{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23757,"status":"ok","timestamp":1707616109782,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"},"user_tz":-540},"id":"eoFso7ArXeHo","outputId":"ed2b245a-0203-4395-9ec7-2eb8db5ebbfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Googleドライブをマウント\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["# カレントディレクトリの変更\n","# %cd drive/MyDrive/BERT_固有表現抽出"],"metadata":{"id":"torbXKts_wm4","executionInfo":{"status":"ok","timestamp":1707616109782,"user_tz":-540,"elapsed":6,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43134,"status":"ok","timestamp":1707616152911,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"},"user_tz":-540},"id":"L1VKNWZ6DNkT","outputId":"db6b811b-2290-441b-efed-6f6939951c0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.18.0\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fugashi==1.1.0\n","  Downloading fugashi-1.1.0.tar.gz (336 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.9/336.9 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ipadic==1.0.0\n","  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytorch-lightning==1.6.1\n","  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.5/582.5 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2.31.0)\n","Collecting sacremoses (from transformers==4.18.0)\n","  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18.0)\n","  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (4.66.1)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1) (2.1.0+cu121)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1) (2023.6.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1) (2.15.1)\n","Collecting torchmetrics>=0.4.1 (from pytorch-lightning==1.6.1)\n","  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1 (from pytorch-lightning==1.6.1)\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1) (4.9.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (3.9.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.60.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.5.2)\n","Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2024.2.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.1) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.1) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.1) (2.1.0)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.6.1)\n","  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (1.3.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (4.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.*->pytorch-lightning==1.6.1) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.2.2)\n","Building wheels for collected packages: fugashi, ipadic\n","  Building wheel for fugashi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fugashi: filename=fugashi-1.1.0-cp310-cp310-linux_x86_64.whl size=257505 sha256=199dccecf4880edb00a3836666314d0bbd9e6cb3f9c5b6c863732756e46ccfac\n","  Stored in directory: /root/.cache/pip/wheels/f6/bc/9b/6d38a64c5bea6582a87574f5c63b6c0bd9a4f5a6706ed577a5\n","  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=c17a1fd5e16572404b40ed1bdbb581a40f19a972b3de50c661c478f77f93a590\n","  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n","Successfully built fugashi ipadic\n","Installing collected packages: tokenizers, ipadic, sacremoses, pyDeprecate, lightning-utilities, fugashi, transformers, torchmetrics, pytorch-lightning\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.1\n","    Uninstalling tokenizers-0.15.1:\n","      Successfully uninstalled tokenizers-0.15.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","Successfully installed fugashi-1.1.0 ipadic-1.0.0 lightning-utilities-0.10.1 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 sacremoses-0.1.1 tokenizers-0.12.1 torchmetrics-1.3.0.post0 transformers-4.18.0\n"]}],"source":["# ライブラリのインストール\n","!pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZCwbcG5Cb9BP","executionInfo":{"status":"ok","timestamp":1707616536479,"user_tz":-540,"elapsed":283,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"outputs":[],"source":["#8-3\n","import itertools\n","import random\n","import json\n","from tqdm import tqdm\n","import numpy as np\n","import unicodedata\n","import pprint\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertForTokenClassification\n","import pytorch_lightning as pl\n","\n","#学習済みモデル\n","MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\""]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jqlQrXEmegzp","executionInfo":{"status":"ok","timestamp":1707616538675,"user_tz":-540,"elapsed":265,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"outputs":[],"source":["# 『BERTによる自然言語処理入門』に記載のプログラム\n","class NER_tokenizer(BertJapaneseTokenizer):\n","\n","    def encode_plus_tagged(self, text, entities, max_length):\n","        \"\"\"\n","        文章とそれに含まれる固有表現が与えられた時に、\n","        符号化とラベル列の作成を行う。\n","        \"\"\"\n","        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n","        entities = sorted(entities, key=lambda x: x['span'][0])\n","        splitted = [] # 分割後の文字列を追加していく\n","        position = 0\n","        for entity in entities:\n","            start = entity['span'][0]\n","            end = entity['span'][1]\n","            label = entity['type_id']\n","            # 固有表現ではないものには0のラベルを付与\n","            splitted.append({'text':text[position:start], 'label':0})\n","            # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n","            splitted.append({'text':text[start:end], 'label':label})\n","            position = end\n","        splitted.append({'text': text[position:], 'label':0})\n","        splitted = [ s for s in splitted if s['text'] ] # 長さ0の文字列は除く\n","\n","        # 分割されたそれぞれの文字列をトークン化し、ラベルをつける。\n","        tokens = [] # トークンを追加していく\n","        labels = [] # トークンのラベルを追加していく\n","        for text_splitted in splitted:\n","            text = text_splitted['text']\n","            label = text_splitted['label']\n","            tokens_splitted = self.tokenize(text)\n","            labels_splitted = [label] * len(tokens_splitted)\n","            tokens.extend(tokens_splitted)\n","            labels.extend(labels_splitted)\n","\n","        # 符号化を行いBERTに入力できる形式にする。\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        encoding = self.prepare_for_model(\n","            input_ids,\n","            max_length=max_length,\n","            padding='max_length',\n","            truncation=True\n","        ) # input_idsをencodingに変換\n","        # 特殊トークン[CLS]、[SEP]のラベルを0にする。\n","        labels = [0] + labels[:max_length-2] + [0]\n","        # 特殊トークン[PAD]のラベルを0にする。\n","        labels = labels + [0]*( max_length - len(labels) )\n","        encoding['labels'] = labels\n","\n","        return encoding\n","\n","    def encode_plus_untagged(\n","        self, text, max_length=None, return_tensors=None\n","    ):\n","        \"\"\"\n","        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n","        \"\"\"\n","        # 文章のトークン化を行い、\n","        # それぞれのトークンと文章中の文字列を対応づける。\n","        tokens = [] # トークンを追加していく。\n","        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n","        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n","        for word in words:\n","            # 単語をサブワードに分割\n","            tokens_word = self.subword_tokenizer.tokenize(word)\n","            tokens.extend(tokens_word)\n","            if tokens_word[0] == '[UNK]': # 未知語への対応\n","                tokens_original.append(word)\n","            else:\n","                tokens_original.extend([\n","                    token.replace('##','') for token in tokens_word\n","                ])\n","\n","        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n","        position = 0\n","        spans = [] # トークンの位置を追加していく。\n","        for token in tokens_original:\n","            l = len(token)\n","            while 1:\n","                if token != text[position:position+l]:\n","                    position += 1\n","                else:\n","                    spans.append([position, position+l])\n","                    position += l\n","                    break\n","\n","        # 符号化を行いBERTに入力できる形式にする。\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        encoding = self.prepare_for_model(\n","            input_ids,\n","            max_length=max_length,\n","            padding='max_length' if max_length else False,\n","            truncation=True if max_length else False\n","        )\n","        sequence_length = len(encoding['input_ids'])\n","        # 特殊トークン[CLS]に対するダミーのspanを追加。\n","        spans = [[-1, -1]] + spans[:sequence_length-2]\n","        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n","        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) )\n","\n","        # 必要に応じてtorch.Tensorにする。\n","        if return_tensors == 'pt':\n","            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n","\n","        return encoding, spans\n","\n","    def convert_bert_output_to_entities(self, text, labels, spans):\n","        \"\"\"\n","        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n","        \"\"\"\n","        # labels, spansから特殊トークンに対応する部分を取り除く\n","        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n","        spans = [span for span in spans if span[0] != -1]\n","\n","        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n","        entities = []\n","        for label, group \\\n","            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n","\n","            group = list(group)\n","            start = spans[group[0][0]][0]\n","            end = spans[group[-1][0]][1]\n","\n","            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n","                entity = {\n","                    \"name\": text[start:end],\n","                    \"span\": [start, end],\n","                    \"type_id\": label\n","                }\n","                entities.append(entity)\n","\n","        return entities"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"vNYgKHjzIxdi","executionInfo":{"status":"ok","timestamp":1707616542604,"user_tz":-540,"elapsed":283,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"outputs":[],"source":["#8-14\n","#データロード\n","load_data_name = 'asec_unique_tag.json'\n","dataset = json.load(open(f'/content/drive/MyDrive/bert_uniqueWord/asection_csv/{load_data_name}','r'))\n","\n","type_id_dict = {\n","    \"技術語\": 1,\n","}\n","\n","#カテゴリをラベルに、テキストの正規化\n","for sample in dataset:\n","  sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n","  for e in sample[\"entities\"]:\n","    e['type_id'] = type_id_dict[e['type']]\n","    del e['type']\n","\n","#データセットの分割\n","random.seed(314)\n","random.shuffle(dataset)\n","n = len(dataset)\n","n_train = int(n*0.6)\n","n_val = int(n*0.2)\n","dataset_train = dataset[:n_train] #学習用\n","dataset_val = dataset[n_train:n_train+n_val]  #検証用\n","dataset_test = dataset[n_train+n_val:] #テスト用"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403},"executionInfo":{"elapsed":40756,"status":"error","timestamp":1707616585863,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"},"user_tz":-540},"id":"1I5avywYLkbG","outputId":"92144ec1-55c5-4f85-d7c3-b3a81ebf1720"},"outputs":[{"output_type":"error","ename":"OSError","evalue":"Can't load tokenizer for 'cl-tohoku/bert-base-japanese-whole-word-masking'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'cl-tohoku/bert-base-japanese-whole-word-masking' is the correct path to a directory containing all relevant files for a NER_tokenizer tokenizer.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-eecd6c3c72f5>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_for_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNER_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#データセットの作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1765\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'cl-tohoku/bert-base-japanese-whole-word-masking'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'cl-tohoku/bert-base-japanese-whole-word-masking' is the correct path to a directory containing all relevant files for a NER_tokenizer tokenizer."]}],"source":["#8-15\n","def create_dataset(tokenizer, dataset, max_length):\n","  dataset_for_loader = []\n","  for sample in dataset:\n","    text = sample['text']\n","    entities = sample['entities']\n","    encoding = tokenizer.encode_plus_tagged(\n","        text, entities, max_length=max_length\n","    )\n","    encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n","    dataset_for_loader.append(encoding)\n","  return dataset_for_loader\n","\n","tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n","\n","#データセットの作成\n","max_length = 256\n","dataset_train_for_loader = create_dataset(\n","    tokenizer, dataset_train, max_length\n",")\n","dataset_val_for_loader = create_dataset(\n","    tokenizer, dataset_val, max_length\n",")\n","\n","#データローダの作成\n","dataloader_train = DataLoader(\n","    dataset_train_for_loader, batch_size=32, shuffle=True\n",")\n","dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"446u47Os_Arh","executionInfo":{"status":"aborted","timestamp":1707616209609,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"outputs":[],"source":["# 8-16\n","# PyTorch Lightningのモデル\n","class BertForTokenClassification_pl(pl.LightningModule):\n","\n","    def __init__(self, model_name, num_labels, lr):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.bert_tc = BertForTokenClassification.from_pretrained(\n","            model_name,\n","            num_labels=num_labels\n","        )\n","\n","    def training_step(self, batch, batch_idx):\n","        output = self.bert_tc(**batch)\n","        loss = output.loss\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert_tc(**batch)\n","        val_loss = output.loss\n","        self.log('val_loss', val_loss)\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n","\n","checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model/'\n",")\n","\n","trainer = pl.Trainer(\n","    gpus=1,\n","    max_epochs=5,\n","    callbacks=[checkpoint]\n",")\n","\n","# ファインチューニング\n","model = BertForTokenClassification_pl(\n","    MODEL_NAME, num_labels=2, lr=1e-5\n",")\n","trainer.fit(model, dataloader_train, dataloader_val)\n","best_model_path = checkpoint.best_model_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OU3oPuxCEWz","executionInfo":{"status":"aborted","timestamp":1707616209610,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"outputs":[],"source":["# 8-17\n","def predict(text, tokenizer, bert_tc):\n","    \"\"\"\n","    BERTで固有表現抽出を行うための関数。\n","    \"\"\"\n","    # 符号化\n","    encoding, spans = tokenizer.encode_plus_untagged(\n","        text, return_tensors='pt'\n","    )\n","    encoding = { k: v.cuda() for k, v in encoding.items() }\n","\n","    # ラベルの予測値の計算\n","    with torch.no_grad():\n","        output = bert_tc(**encoding)\n","        scores = output.logits\n","        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n","\n","    # ラベル列を固有表現に変換\n","    entities = tokenizer.convert_bert_output_to_entities(\n","        text, labels_predicted, spans\n","    )\n","\n","    return entities\n","\n","# トークナイザのロード\n","tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n","\n","# ファインチューニングしたモデルをロードし、GPUにのせる。\n","model = BertForTokenClassification_pl.load_from_checkpoint(\n","    best_model_path\n",")\n","bert_tc = model.bert_tc.cuda()\n","\n","# 固有表現抽出\n","# 注：以下ではコードのわかりやすさのために、1データづつ処理しているが、\n","# バッチ化して処理を行った方が処理時間は短い\n","entities_list = [] # 正解の固有表現を追加していく。\n","entities_predicted_list = [] # 抽出された固有表現を追加していく。\n","for sample in tqdm(dataset_test):\n","    text = sample['text']\n","    entities_predicted = predict(text, tokenizer, bert_tc) # BERTで予測\n","    entities_list.append(sample['entities'])\n","    entities_predicted_list.append( entities_predicted )\n","    #print(text)\n","    #print(entities_predicted)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWfDSvX6ad4x","executionInfo":{"status":"aborted","timestamp":1707616209611,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"outputs":[],"source":["print(\"# 正解\")\n","print(entities_list[0])\n","print(\"# 抽出\")\n","print(entities_predicted_list[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JeZzmNigiDsD","executionInfo":{"status":"aborted","timestamp":1707616209611,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"outputs":[],"source":["# 8-19\n","def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n","    \"\"\"\n","    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n","    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n","    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n","    \"\"\"\n","    num_entities = 0 # 固有表現(正解)の個数\n","    num_predictions = 0 # BERTにより予測された固有表現の個数\n","    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n","\n","    # それぞれの文章で予測と正解を比較。\n","    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n","    for entities, entities_predicted \\\n","        in zip(entities_list, entities_predicted_list):\n","\n","        if type_id:\n","            entities = [ e for e in entities if e['type_id'] == type_id ]\n","            entities_predicted = [\n","                e for e in entities_predicted if e['type_id'] == type_id\n","            ]\n","\n","        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n","        set_entities = set( get_span_type(e) for e in entities )\n","        set_entities_predicted = \\\n","            set( get_span_type(e) for e in entities_predicted )\n","\n","        num_entities += len(entities)\n","        num_predictions += len(entities_predicted)\n","        num_correct += len( set_entities & set_entities_predicted )\n","\n","    # 指標を計算\n","    precision = num_correct/num_predictions # 適合率\n","    recall = num_correct/num_entities # 再現率\n","    f_value = 2*precision*recall/(precision+recall) # F値\n","\n","    result = {\n","        'num_entities': num_entities,\n","        'num_predictions': num_predictions,\n","        'num_correct': num_correct,\n","        'precision': precision,\n","        'recall': recall,\n","        'f_value': f_value\n","    }\n","\n","    return result"]},{"cell_type":"markdown","source":["![image.png](data:image/png;base64,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)"],"metadata":{"id":"tp3Y2tN7FeUb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTkJaz0GLG5T","executionInfo":{"status":"aborted","timestamp":1707616209612,"user_tz":-540,"elapsed":13,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"outputs":[],"source":["#8-20\n","#性能評価\n","pprint.pprint(evaluate_model(entities_list, entities_predicted_list), indent=3, sort_dicts=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"duBQSejb_Tgy","executionInfo":{"status":"aborted","timestamp":1707616209612,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tsukito Mizoguchi","userId":"08849339837224803129"}}},"outputs":[],"source":["# 抽出データ比較用CSV生成\n","import pandas as pd\n","n = 0\n","list_for_df = []\n","for predictions, corrections, sample in zip(entities_predicted_list, entities_list, dataset_test):\n","  n += 1\n","  s_name = sample['text']\n","  p_lst = []\n","  c_lst = []\n","  for p in predictions:\n","    p_lst.append(p['name'])\n","  for c in corrections:\n","    c_lst.append(c['name'])\n","  while len(c_lst) != len(p_lst):\n","    if len(c_lst) > len(p_lst):\n","      p_lst.append(\"[hoge]\")\n","    else:\n","      c_lst.append(\"[hoge]\")\n","  i = 0\n","  for c_token, p_token in zip(c_lst, p_lst):\n","    dict_for_df = {}\n","    i += 1\n","    t_name = \"token\" + str(i)\n","    dict_for_df['sentence'] = s_name\n","    dict_for_df['token'] = t_name\n","    dict_for_df['correct'] = c_token\n","    dict_for_df['predicted'] = p_token\n","    list_for_df.append(dict_for_df)\n","df = pd.DataFrame(list_for_df)\n","df.to_csv(\"hsec_predict.csv\", index=None, encoding=\"shift-jis\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"14WwUXs2QqeA3qEChai-dxK7hSOHohwSv","timestamp":1684750913180}],"authorship_tag":"ABX9TyPDV7hLW8UcFIG6+KJ+9/Is"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}